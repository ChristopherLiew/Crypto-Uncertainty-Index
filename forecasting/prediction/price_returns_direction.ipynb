{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BTC-USD Price Returns Direction Prediction\n",
    "1. Approach 1: Based on Forecasted Value by our Existing Model\n",
    "   1. Data Preparation\n",
    "   2. Classification\n",
    "   3. Evaluation\n",
    "      1. Classification Error\n",
    "      2. Better than 0.5 (Coin Flip)?\n",
    "\n",
    "\n",
    "2. [KIV] Approach 2: Train a classifier using Market and Uncertainty Indices data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/christopherliew/Desktop/Y4S1/HT/crypto_uncertainty_index\n"
     ]
    }
   ],
   "source": [
    "# NB config\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Load Libraries\n",
    "import os\n",
    "import importlib\n",
    "import warnings\n",
    "os.chdir(\"../../\")\n",
    "print(os.getcwd())\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "from pathlib import Path\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "dm_test = importlib.import_module(\"Diebold-Mariano-Test.dm_test\")\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Approach 1\n",
    "\n",
    "#### Data Preparation\n",
    "* Generate Price Returns for h = 1 to 12 (Y-Actual)\n",
    "* Get Forecasts for h = 1 to 12 (Y-Pred)\n",
    "* Create 12 dataframes for each pair of (Y-Actual, Y-Pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_log_price_returns(\n",
    "    series: pd.DataFrame, h: int, var_col: str = \"Price\", drop_cols: List[str] = ['Volume']\n",
    ") -> pd.DataFrame:\n",
    "    new_col_name = \"price_return\"\n",
    "    series[new_col_name] = np.log1p(series[[var_col]].pct_change(h))\n",
    "    drop_cols.append(var_col)\n",
    "    return series.drop(columns=drop_cols).rename(columns={'Date': 'time'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Data\n",
    "# Data Dir\n",
    "data_dir = Path(\"forecasting/data/modelling\")\n",
    "\n",
    "# BTC-USD data\n",
    "btc_usd_fp = data_dir / \"btc_usd_weekly.csv\"\n",
    "btc_usd_df = pd.read_csv(btc_usd_fp)\n",
    "\n",
    "# Generate Log Price Returns for multi-horizons\n",
    "y_actual_dict = {}\n",
    "\n",
    "for i in range(0, 12):\n",
    "    y_actual_dict[i] = (\n",
    "        gen_log_price_returns(btc_usd_df, i + 1)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate Directions\n",
    "\n",
    "y_actual_clf_dict = {}\n",
    "\n",
    "for i, df in y_actual_dict.items():\n",
    "    df['direction'] = df['price_return'].apply(lambda x: 'positive' if x >= 0 else 'negative')\n",
    "    y_actual_clf_dict[i] = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Predictions for h = 1 to 12\n",
    "forecast_dir = Path(\"forecasting/data/forecasts/random_forest\")\n",
    "\n",
    "def get_forecast_fps(dir: Path, keyword: str = 'rf_model') -> List[Path]:\n",
    "    return (\n",
    "        list(filter(lambda x: keyword in str(x), list(dir.glob(\"*.csv\"))))\n",
    "    )\n",
    "\n",
    "# Model A\n",
    "model_A_dir = forecast_dir / \"model_A\"\n",
    "model_A_forecast_dict = {\n",
    "    k: pd.read_csv(fp) for k, fp\n",
    "    in enumerate(get_forecast_fps(model_A_dir))\n",
    "}\n",
    "\n",
    "# Model B\n",
    "model_B_dir = forecast_dir / \"model_B\"\n",
    "model_B_forecast_dict = {\n",
    "    k: pd.read_csv(fp) for k, fp\n",
    "    in enumerate(get_forecast_fps(model_B_dir))\n",
    "}\n",
    "\n",
    "# Model C\n",
    "model_C_dir = forecast_dir / \"model_C\"\n",
    "model_C_forecast_dict = {\n",
    "    k: pd.read_csv(fp) for k, fp\n",
    "    in enumerate(get_forecast_fps(model_C_dir))\n",
    "}\n",
    "\n",
    "# Model D\n",
    "model_D_dir = forecast_dir / \"model_D\"\n",
    "model_D_forecast_dict = {\n",
    "    k: pd.read_csv(fp) for k, fp\n",
    "    in enumerate(get_forecast_fps(model_D_dir))\n",
    "}\n",
    "\n",
    "# Model E\n",
    "model_E_dir = forecast_dir / \"model_E\"\n",
    "model_E_forecast_dict = {\n",
    "    k: pd.read_csv(fp) for k, fp\n",
    "    in enumerate(get_forecast_fps(model_E_dir))\n",
    "}\n",
    "\n",
    "# Model F\n",
    "model_F_dir = forecast_dir / \"model_F\"\n",
    "model_F_forecast_dict = {\n",
    "    k: pd.read_csv(fp) for k, fp\n",
    "    in enumerate(get_forecast_fps(model_F_dir))\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process Forecast Dict helper\n",
    "\n",
    "def classify_multi_h(forecast_dict: Dict[int, pd.DataFrame], col_name: str = 'price_return', threshold: float = 0.0):\n",
    "    results = {}\n",
    "    for k, df in forecast_dict.items():\n",
    "        df['direction'] = df[col_name].apply(lambda x: 'positive' if x >= threshold else 'negative')\n",
    "        results[k] = df\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute classification results\n",
    "\n",
    "def get_clf_res(truth_dict, pred_dict, clf_col: str = 'direction'):\n",
    "    assert truth_dict.keys() == pred_dict.keys()\n",
    "    reports = {}\n",
    "    for i in list(truth_dict.keys()):\n",
    "        combined_df = pred_dict[i].merge(truth_dict[i], on='time', suffixes=('_pred', '_truth')).dropna()\n",
    "        reports[i + 1] = pd.DataFrame(\n",
    "            classification_report(combined_df[clf_col + '_pred'], combined_df[clf_col + '_truth'], output_dict=True)\n",
    "        )\n",
    "    return reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.280702   0.661972  0.492188    0.471337      0.542825\n",
      "recall      0.400000   0.534091  0.492188    0.467045      0.492188\n",
      "f1-score    0.329897   0.591195  0.492188    0.460546      0.509539\n",
      "support    40.000000  88.000000  0.492188  128.000000    128.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.418182   0.666667  0.559055    0.542424      0.574708\n",
      "recall      0.489362   0.600000  0.559055    0.544681      0.559055\n",
      "f1-score    0.450980   0.631579  0.559055    0.541280      0.564743\n",
      "support    47.000000  80.000000  0.559055  127.000000    127.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.681818   0.786517   0.75188    0.734168      0.747944\n",
      "recall      0.612245   0.833333   0.75188    0.722789      0.751880\n",
      "f1-score    0.645161   0.809249   0.75188    0.727205      0.748795\n",
      "support    49.000000  84.000000   0.75188  133.000000    133.000000\n"
     ]
    }
   ],
   "source": [
    "# Model A\n",
    "model_A_clf = classify_multi_h(\n",
    "    model_A_forecast_dict\n",
    ")\n",
    "\n",
    "model_A_results = get_clf_res(y_actual_clf_dict, model_A_clf)\n",
    "\n",
    "# Inspect Results for h = 1, 4 and 12\n",
    "print(model_A_results[1])\n",
    "print(model_A_results[4])\n",
    "print(model_A_results[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.321429   0.591549  0.472441    0.456489      0.491583\n",
      "recall      0.382979   0.525000  0.472441    0.453989      0.472441\n",
      "f1-score    0.349515   0.556291  0.472441    0.452903      0.479768\n",
      "support    47.000000  80.000000  0.472441  127.000000    127.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.385965   0.794521  0.615385    0.590243      0.678239\n",
      "recall      0.594595   0.623656  0.615385    0.609125      0.615385\n",
      "f1-score    0.468085   0.698795  0.615385    0.583440      0.633132\n",
      "support    37.000000  93.000000  0.615385  130.000000    130.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.863636   0.952381  0.921875    0.908009      0.923262\n",
      "recall      0.904762   0.930233  0.921875    0.917497      0.921875\n",
      "f1-score    0.883721   0.941176  0.921875    0.912449      0.922324\n",
      "support    42.000000  86.000000  0.921875  128.000000    128.000000\n"
     ]
    }
   ],
   "source": [
    "# Model B\n",
    "model_B_clf = classify_multi_h(\n",
    "    model_B_forecast_dict\n",
    ")\n",
    "\n",
    "model_B_results = get_clf_res(y_actual_clf_dict, model_B_clf)\n",
    "\n",
    "# Inspect Results for h = 1, 4 and 12\n",
    "print(model_B_results[1])\n",
    "print(model_B_results[4])\n",
    "print(model_B_results[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.321429   0.608696      0.48    0.465062      0.505280\n",
      "recall      0.400000   0.525000      0.48    0.462500      0.480000\n",
      "f1-score    0.356436   0.563758      0.48    0.460097      0.489122\n",
      "support    45.000000  80.000000      0.48  125.000000    125.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.388889   0.694444  0.563492    0.541667      0.590168\n",
      "recall      0.488372   0.602410  0.563492    0.545391      0.563492\n",
      "f1-score    0.432990   0.645161  0.563492    0.539075      0.572754\n",
      "support    43.000000  83.000000  0.563492  126.000000    126.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.363636   0.673913  0.573529    0.518775      0.568967\n",
      "recall      0.347826   0.688889  0.573529    0.518357      0.573529\n",
      "f1-score    0.355556   0.681319  0.573529    0.518437      0.571134\n",
      "support    46.000000  90.000000  0.573529  136.000000    136.000000\n"
     ]
    }
   ],
   "source": [
    "# Model C\n",
    "model_C_clf = classify_multi_h(\n",
    "    model_C_forecast_dict\n",
    ")\n",
    "\n",
    "model_C_results = get_clf_res(y_actual_clf_dict, model_C_clf)\n",
    "\n",
    "# Inspect Results for h = 1, 4 and 12\n",
    "print(model_C_results[1])\n",
    "print(model_C_results[4])\n",
    "print(model_C_results[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.372881   0.630137  0.515152    0.501509      0.534641\n",
      "recall      0.448980   0.554217  0.515152    0.501598      0.515152\n",
      "f1-score    0.407407   0.589744  0.515152    0.498575      0.522058\n",
      "support    49.000000  83.000000  0.515152  132.000000    132.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.385965   0.794521  0.615385    0.590243      0.678239\n",
      "recall      0.594595   0.623656  0.615385    0.609125      0.615385\n",
      "f1-score    0.468085   0.698795  0.615385    0.583440      0.633132\n",
      "support    37.000000  93.000000  0.615385  130.000000    130.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.818182   0.894118  0.868217    0.856150      0.867628\n",
      "recall      0.800000   0.904762  0.868217    0.852381      0.868217\n",
      "f1-score    0.808989   0.899408  0.868217    0.854199      0.867867\n",
      "support    45.000000  84.000000  0.868217  129.000000    129.000000\n"
     ]
    }
   ],
   "source": [
    "# Model D\n",
    "model_D_clf = classify_multi_h(\n",
    "    model_D_forecast_dict\n",
    ")\n",
    "\n",
    "model_D_results = get_clf_res(y_actual_clf_dict, model_D_clf)\n",
    "\n",
    "# Inspect Results for h = 1, 4 and 12\n",
    "print(model_D_results[1])\n",
    "print(model_D_results[4])\n",
    "print(model_D_results[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.315789   0.625000  0.488372    0.470395      0.517136\n",
      "recall      0.400000   0.535714  0.488372    0.467857      0.488372\n",
      "f1-score    0.352941   0.576923  0.488372    0.464932      0.498790\n",
      "support    45.000000  84.000000  0.488372  129.000000    129.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.370370   0.647887     0.528    0.509129      0.547981\n",
      "recall      0.444444   0.575000     0.528    0.509722      0.528000\n",
      "f1-score    0.404040   0.609272     0.528    0.506656      0.535388\n",
      "support    45.000000  80.000000     0.528  125.000000    125.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.545455   0.788889  0.708955    0.667172      0.710772\n",
      "recall      0.558140   0.780220  0.708955    0.669180      0.708955\n",
      "f1-score    0.551724   0.784530  0.708955    0.668127      0.709824\n",
      "support    43.000000  91.000000  0.708955  134.000000    134.000000\n"
     ]
    }
   ],
   "source": [
    "# Model E\n",
    "model_E_clf = classify_multi_h(\n",
    "    model_E_forecast_dict\n",
    ")\n",
    "\n",
    "model_E_results = get_clf_res(y_actual_clf_dict, model_E_clf)\n",
    "\n",
    "# Inspect Results for h = 1, 4 and 12\n",
    "print(model_E_results[1])\n",
    "print(model_E_results[4])\n",
    "print(model_E_results[12])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            negative    positive  accuracy   macro avg  weighted avg\n",
      "precision   0.216667    0.723684       0.5    0.470175      0.596930\n",
      "recall      0.382353    0.539216       0.5    0.460784      0.500000\n",
      "f1-score    0.276596    0.617978       0.5    0.447287      0.532632\n",
      "support    34.000000  102.000000       0.5  136.000000    136.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.578947   0.810811  0.709924    0.694879      0.727623\n",
      "recall      0.702128   0.714286  0.709924    0.708207      0.709924\n",
      "f1-score    0.634615   0.759494  0.709924    0.697055      0.714690\n",
      "support    47.000000  84.000000  0.709924  131.000000    131.000000\n",
      "            negative   positive  accuracy   macro avg  weighted avg\n",
      "precision   0.909091   0.888889     0.896    0.898990      0.896808\n",
      "recall      0.816327   0.947368     0.896    0.881847      0.896000\n",
      "f1-score    0.860215   0.917197     0.896    0.888706      0.894860\n",
      "support    49.000000  76.000000     0.896  125.000000    125.000000\n"
     ]
    }
   ],
   "source": [
    "# Model F\n",
    "model_F_clf = classify_multi_h(\n",
    "    model_F_forecast_dict\n",
    ")\n",
    "\n",
    "model_F_results = get_clf_res(y_actual_clf_dict, model_F_clf)\n",
    "\n",
    "# Inspect Results for h = 1, 4 and 12\n",
    "print(model_F_results[1])\n",
    "print(model_F_results[4])\n",
    "print(model_F_results[12])"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "26fa31afa754fb971b44a94a9612d34d26997a00d823853c42f1b189c287d4b1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.12 ('crypto-uncertainty-index-SX872G0E-py3.8': poetry)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
